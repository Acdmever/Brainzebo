{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyedflib\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import sklearn\n",
    "import sklearn.multioutput\n",
    "import sklearn.ensemble\n",
    "import sklearn.neighbors\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makecsv(participant,dates,refresh):\n",
    "    #values=[]\n",
    "    for session in range(1,4):\n",
    "        curr=classes[:,session-1]\n",
    "\n",
    "        #Read self assesments (valence-arousal)\n",
    "        file=open('Dataset_1/Common/Part'+participant+'SES'+str(session)+'.log','r')\n",
    "        score=re.split('\\n\\n|\\n',file.read())\n",
    "        i=0\n",
    "        scores=[]\n",
    "        for s in score:\n",
    "            i=i+1\n",
    "            if i%5==0:\n",
    "                scores.append(list(map(float,(s.split(' '))[-2:])))\n",
    "        scores=np.asarray(scores)\n",
    "\n",
    "        #Read markers\n",
    "        file=open('Dataset_1/EEG/Part'+participant+'_IAPS_SES'+str(session)+'_EEG_fNIRS_0'+dates[session-1]+'082006.bdf.mrk','r')\n",
    "        tags=file.read().split('\\t')\n",
    "        tag=[]\n",
    "        i=0\n",
    "        for t in tags[:-3]:\n",
    "            if t[0].isdigit():\n",
    "                i=i+1\n",
    "                if i%4==0:\n",
    "                    tag.append(int(t))\n",
    "\n",
    "        marks=(sorted(set(tag)))\n",
    "\n",
    "        #Read CSV\n",
    "        df=pd.read_csv('Dataset_1/EEG/CSV/Part'+participant+'_IAPS_SES'+str(session)+'_EEG_fNIRS_0'+dates[session-1]+'082006.csv')\n",
    "        \n",
    "        #Take only these channels since Emotiv uses only these\n",
    "        df=df[['AF3', 'FT7','F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'FT8', 'AF4']]\n",
    "\n",
    "        #Get FFTs jointly of the first 3 seconds of each trigger. We do this every 2 seconds\n",
    "        rate=refresh[session-1]\n",
    "        i=marks[0]\n",
    "        for m in marks:\n",
    "            i=m\n",
    "            ind=marks.index(m)\n",
    "            for s in range (2):\n",
    "                e=i+refresh[session-1]\n",
    "                d1=df[i:e+2*refresh[session-1]]\n",
    "                arr=np.zeros((14,75))\n",
    "                j=0\n",
    "                for col in list(d1):\n",
    "                    fft=(np.abs(np.fft.hfft(d1[col]))).tolist()\n",
    "                    \n",
    "                    fft=np.asarray(fft[1:76]) #1-25 hz\n",
    "                    arr[j]=fft\n",
    "                    j=j+1\n",
    "\n",
    "\n",
    "                #Flatten array and join them to values\n",
    "                vals=arr.flatten()\n",
    "                vals=vals.tolist()\n",
    "                vals.append(scores[ind][0])\n",
    "                vals.append(scores[ind][1])\n",
    "\n",
    "                values.append(np.asarray(vals).flatten())\n",
    "                i=e\n",
    "\n",
    "#Read Classes of images\n",
    "file=open('Dataset_1/Common/IAPS_Classes_EEG_fNIRS.txt','r')\n",
    "classes=re.split('\\t|\\n',file.read())\n",
    "new=[(classes[:-1])[i:i+3] for i in range(0, len(classes)-1, 3)]\n",
    "classes=np.asarray(new)\n",
    "\n",
    "\n",
    "values=[]\n",
    "#Participants\n",
    "#1\n",
    "dates=['3','7','8']\n",
    "refresh=[256,1024,1024]\n",
    "makecsv('1',dates,refresh)\n",
    "\n",
    "#2\n",
    "dates=['7','8','9']\n",
    "refresh=[1024,1024,1024]\n",
    "makecsv('2',dates,refresh)\n",
    "\n",
    "#3\n",
    "dates=['7','8','8']\n",
    "makecsv('3',dates,refresh)\n",
    "\n",
    "\n",
    "#4\n",
    "dates=['7','8','9']\n",
    "makecsv('4',dates,refresh)\n",
    "\n",
    "\n",
    "#5\n",
    "dates=['8','9','9']\n",
    "makecsv('5',dates,refresh)\n",
    "\n",
    "np.savetxt(\"Dataset1_3SecJoinedFT_Flat.csv\",np.asarray(values),delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATASET 2 FORMATTING\n",
    "values=[]\n",
    "for part in range(1,24):\n",
    "    for vid in range (1,19):\n",
    "        df=pd.read_csv('Dataset_2/MatlabOutput/Part'+str(part)+'Vid'+str(vid)+'.csv',header=None).values\n",
    "        score=pd.read_csv('Dataset_2/MatlabOutput/Part'+str(part)+'Vid'+str(vid)+'Score.csv',header=None).values[0]\n",
    "        \n",
    "        \n",
    "        #Removing DC Offset\n",
    "        #for x in range (df.shape[1]):\n",
    "        #    m=np.mean(df[:,x])\n",
    "        #    df[:,x]=df[:,x]-m\n",
    "        #df=df-4200\n",
    "        \n",
    "        \n",
    "        \n",
    "        start=500\n",
    "        #Get FFTs separately of the first 3 seconds of each trigger. We do this every 2 seconds\n",
    "        for x in range (10):\n",
    "            end=start+128\n",
    "            d1=df[start:end,:]\n",
    "            d2=df[end:end+128,:]\n",
    "            d3=df[end+128:end+256,:]\n",
    "            arr=np.zeros((14,75))\n",
    "            j=0\n",
    "            for col in range(14):\n",
    "                fft1=(np.abs(np.fft.hfft(d1[:,col]))).tolist()\n",
    "                fft2=(np.abs(np.fft.hfft(d2[:,col]))).tolist()\n",
    "                fft3=(np.abs(np.fft.hfft(d3[:,col]))).tolist()\n",
    "                fft=np.asarray(fft1[1:26]+fft2[1:26]+fft3[1:26]) #1-25 hz\n",
    "                \n",
    "                arr[j]=fft\n",
    "                j=j+1\n",
    "                \n",
    "\n",
    "            #Flatten and join\n",
    "            vals=arr.flatten()\n",
    "            #vals=vals/np.sum(vals)\n",
    "            vals=vals.tolist()\n",
    "            vals.append(score[0])\n",
    "            vals.append(score[1])\n",
    "            #vals.append(curr[ind])\n",
    "            values.append(np.asarray(vals).flatten())\n",
    "            start=end    \n",
    "np.savetxt(\"Dataset2_3SecSeparateFT_FlatWithOS.csv\",np.asarray(values),delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells below are just a thorough test of the datasets with different types of classifiers. Doing this, I chose the best formatting for the datasets and the best classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Training classifier\n",
    "\n",
    "#Reading dataset file\n",
    "dataset=pd.read_csv('Dataset2_3SecJoinedFT_FlatOS.csv',header=None).values\n",
    "\n",
    "\n",
    "x=dataset[:,:-2]\n",
    "#x=x*1000\n",
    "y=dataset[:,-2:]\n",
    "\n",
    "print(x.shape)\n",
    "x,y=shuffle(x,y)\n",
    "train=x[:3000]\n",
    "test=x[3000:]\n",
    "\n",
    "y1=y[:,0].astype(int)\n",
    "y2=y[:,1].astype(int)\n",
    "y1train=y1[:3000].astype(int)\n",
    "y1test=y1[3000:].astype(int)\n",
    "y2train=y2[:3000].astype(int)\n",
    "y2test=y2[3000:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEST\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Reading dataset file\n",
    "dataset=pd.read_csv('Dataset2_3SecSeparateFT_FlatWithOS.csv',header=None).values\n",
    "\n",
    "\n",
    "x=dataset[:,:-2]\n",
    "x=x*5000\n",
    "y=dataset[:,-2:]\n",
    "\n",
    "print(x.shape)\n",
    "x,y=shuffle(x,y)\n",
    "train=x[:500]\n",
    "test=x[500:]\n",
    "\n",
    "y1=y[:,0].astype(int)\n",
    "y2=y[:,1].astype(int)\n",
    "y1train=y1[:500].astype(int)\n",
    "y1test=y1[500:].astype(int)\n",
    "y2train=y2[:500].astype(int)\n",
    "y2test=y2[500:].astype(int)\n",
    "print ('3 Sec Separate')\n",
    "mlr1=MLPClassifier(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "mlr2=MLPClassifier(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "print(np.mean(cross_val_score(mlr1,x,y1,cv=3)))\n",
    "print(np.mean(cross_val_score(mlr2,x,y2,cv=3)))\n",
    "mlr1.fit(train,y1train)\n",
    "mlr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(mlr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(mlr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if p1==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (p1-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if p2==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (p2-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Reading dataset file\n",
    "dataset=pd.read_csv('Dataset2_3SecJoinedFT_Flat.csv',header=None).values\n",
    "\n",
    "\n",
    "x=dataset[:,:-2]\n",
    "x=x*5000\n",
    "y=dataset[:,-2:]\n",
    "\n",
    "print(x.shape)\n",
    "x,y=shuffle(x,y)\n",
    "train=x[:500]\n",
    "test=x[500:]\n",
    "\n",
    "y1=y[:,0].astype(int)\n",
    "y2=y[:,1].astype(int)\n",
    "y1train=y1[:500].astype(int)\n",
    "y1test=y1[500:].astype(int)\n",
    "y2train=y2[:500].astype(int)\n",
    "y2test=y2[500:].astype(int)\n",
    "print ('\\n\\n3 Sec Joined')\n",
    "mlr1=MLPClassifier(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "mlr2=MLPClassifier(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "print(np.mean(cross_val_score(mlr1,x,y1,cv=3)))\n",
    "print(np.mean(cross_val_score(mlr2,x,y2,cv=3)))\n",
    "mlr1.fit(train,y1train)\n",
    "mlr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(mlr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(mlr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if p1==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (p1-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if p2==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (p2-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=sklearn.multioutput.MultiOutputClassifier(sklearn.svm.SVC(gamma='auto'), n_jobs=2)\n",
    "np.mean(cross_val_score(clf,x,y,cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm1=sklearn.svm.SVC(gamma='auto')\n",
    "print(np.mean(cross_val_score(svm1,x,y1,cv=10)))\n",
    "print(np.mean(cross_val_score(svm1,x,y2,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=sklearn.ensemble.RandomForestClassifier(n_estimators=50,criterion='entropy',max_depth=400)\n",
    "print(np.mean(cross_val_score(rf,x,y1,cv=10)))\n",
    "print(np.mean(cross_val_score(rf,x,y2,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=linear_model.LogisticRegression(solver='saga',max_iter=4000,multi_class='multinomial')\n",
    "print(np.mean(cross_val_score(lr,x,y1,cv=10)))\n",
    "print(np.mean(cross_val_score(lr,x,y2,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=sklearn.neighbors.KNeighborsClassifier(n_neighbors=5,algorithm='auto')\n",
    "print(np.mean(cross_val_score(knn,x,y1,cv=10)))\n",
    "print(np.mean(cross_val_score(knn,x,y2,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp1=MLPClassifier(solver='adam',activation='relu', alpha=1e-3,hidden_layer_sizes=(600,),max_iter=1000, random_state=0)\n",
    "mlp2=MLPClassifier(solver='adam',activation='relu', alpha=1e-3,hidden_layer_sizes=(600,),max_iter=1000, random_state=0)\n",
    "print(np.mean(cross_val_score(mlp1,x,y1,cv=10)))\n",
    "print(np.mean(cross_val_score(mlp2,x,y2,cv=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr1=sklearn.svm.SVR(kernel='poly',gamma='scale')\n",
    "svr2=sklearn.svm.SVR(kernel='poly',gamma='scale')\n",
    "svr1.fit(train,y1train)\n",
    "svr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(svr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(svr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid1=sklearn.linear_model.Ridge(alpha=1.0)\n",
    "rid2=sklearn.linear_model.Ridge(alpha=1.0)\n",
    "rid1.fit(train,y1train)\n",
    "rid2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(rid1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(rid2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lars1=sklearn.linear_model.LassoLars(alpha=1.0)\n",
    "lars2=sklearn.linear_model.LassoLars(alpha=1.0)\n",
    "lars1.fit(train,y1train)\n",
    "lars2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(lars1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(lars2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay1=sklearn.linear_model.BayesianRidge(n_iter=1000)\n",
    "bay2=sklearn.linear_model.BayesianRidge(n_iter=1000)\n",
    "bay1.fit(train,y1train)\n",
    "bay2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(bay1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(bay2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr1=sklearn.svm.SVR(kernel='sigmoid',gamma='scale')\n",
    "svr2=sklearn.svm.SVR(kernel='sigmoid',gamma='scale')\n",
    "svr1.fit(train,y1train)\n",
    "svr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(svr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(svr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per1=sklearn.linear_model.Perceptron(max_iter=2000)\n",
    "per2=sklearn.linear_model.Perceptron(max_iter=2000)\n",
    "per1.fit(train,y1train)\n",
    "per2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(per1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(per2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if p1==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (p1-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if p2==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (p2-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the1=sklearn.linear_model.TheilSenRegressor()\n",
    "the2=sklearn.linear_model.TheilSenRegressor()\n",
    "the1.fit(train,y1train)\n",
    "the2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(the1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(the2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate further class dimension, try more regressors\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "\n",
    "pas1=PassiveAggressiveRegressor()\n",
    "pas2=PassiveAggressiveRegressor()\n",
    "pas1.fit(train,y1train)\n",
    "pas2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    i=random.randint(0,300)\n",
    "    p1=min(max(pas1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(pas2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd1=SGDRegressor(max_iter=5000,tol=0.01)\n",
    "sgd2=SGDRegressor(max_iter=5000,tol=0.01)\n",
    "sgd1.fit(train,y1train)\n",
    "sgd2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(sgd1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(sgd2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "hub1=HuberRegressor(max_iter=1000)\n",
    "hub2=HuberRegressor(max_iter=1000)\n",
    "hub1.fit(train,y1train)\n",
    "hub2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(hub1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(hub2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn1=KNeighborsRegressor(n_neighbors=5)\n",
    "knn2=KNeighborsRegressor(n_neighbors=5)\n",
    "knn1.fit(train,y1train)\n",
    "knn2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(knn1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(knn2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "kernel = DotProduct() + WhiteKernel()\n",
    "gpr1=GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "gpr2=GaussianProcessRegressor(kernel=kernel,random_state=0)\n",
    "gpr1.fit(train,y1train)\n",
    "gpr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(gpr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(gpr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dec1=tree.DecisionTreeRegressor()\n",
    "dec2=tree.DecisionTreeRegressor()\n",
    "dec1.fit(train,y1train)\n",
    "dec2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(dec1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(dec2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "dec1=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=200, random_state=0, loss='ls')\n",
    "dec2=GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=200, random_state=0, loss='ls')\n",
    "dec1.fit(train,y1train)\n",
    "dec2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(dec1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(dec2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlr1=MLPRegressor(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "mlr2=MLPRegressor(solver='adam',activation='relu',alpha=1e-5,hidden_layer_sizes=(400,),max_iter=1000, random_state=0)\n",
    "mlr1.fit(train,y1train)\n",
    "mlr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(mlr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(mlr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlr1=MLPClassifier(solver='adam',activation='relu',alpha=1e-6,hidden_layer_sizes=(600,),max_iter=2000, random_state=0)\n",
    "mlr2=MLPClassifier(solver='adam',activation='relu',alpha=1e-6,hidden_layer_sizes=(600,),max_iter=2000, random_state=0)\n",
    "mlr1.fit(train,y1train)\n",
    "mlr2.fit(train,y2train)\n",
    "error1=0\n",
    "error2=0\n",
    "for x in range(300):\n",
    "    #i=random.randint(0,300)\n",
    "    p1=min(max(mlr1.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    p2=min(max(mlr2.predict(test[x].reshape(1,-1))[0],1),10)\n",
    "    if round(p1)==y1test[x]:\n",
    "        error1=error1+3\n",
    "    if (round(p1)-y1test[x])**2==1:\n",
    "        error1=error1+1\n",
    "    if round(p2)==y2test[x]:\n",
    "        error2=error2+3\n",
    "    if (round(p2)-y2test[x])**2==1:\n",
    "        error2=error2+1\n",
    "    #error1=error1+(y1test[x]-p1)**2\n",
    "    #error2=error2+(y2test[x]-p2)**2\n",
    "    if x%10==0:\n",
    "        print(y1test[x],y2test[x])\n",
    "        print(p1,p2)\n",
    "        print()\n",
    "print('Valence:',error1)\n",
    "print('Arousal:',error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Receiving data from OpenBCI GUI TEST\n",
    "import argparse\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pythonosc import dispatcher\n",
    "from pythonosc import osc_server\n",
    "from pythonosc import udp_client\n",
    "from pythonosc import osc_message_builder\n",
    "from pythonosc import osc_bundle_builder\n",
    "from typing import List, Any\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Reading and joining both datasets\n",
    "dataset=pd.read_csv('Dataset1_3SecJoinedFT_Flat.csv',header=None).values\n",
    "ds2=pd.read_csv('Dataset2_3SecJoinedFT_FlatOS.csv',header=None).values\n",
    "dataset=np.append(dataset,ds2,axis=0)\n",
    "x=dataset[:,:-2]\n",
    "\n",
    "y=dataset[:,-2:]\n",
    "\n",
    "\n",
    "y1=y[:,0].astype(int)\n",
    "y2=y[:,1].astype(int)\n",
    "\n",
    "\n",
    "#Train classifiers (MLP and KNN)\n",
    "print('Training Classifiers...')\n",
    "mlr1=MLPClassifier(solver='adam',activation='relu',alpha=1e-6,hidden_layer_sizes=(600,),max_iter=2000, random_state=0)\n",
    "mlr2=MLPClassifier(solver='adam',activation='relu',alpha=1e-6,hidden_layer_sizes=(600,),max_iter=2000, random_state=0)\n",
    "mlr1.fit(x,y1)\n",
    "mlr2.fit(x,y2)\n",
    "\n",
    "knn1=KNeighborsRegressor(n_neighbors=5)\n",
    "knn2=KNeighborsRegressor(n_neighbors=5)\n",
    "knn1.fit(x,y1)\n",
    "knn2.fit(x,y2)\n",
    "\n",
    "print('Done!')\n",
    "\n",
    "#Intialize list of Timeseries values\n",
    "vals=[]\n",
    "def print_test(addr,*args:List[Any]):\n",
    "    vals.append(np.asarray(args)[2:])\n",
    "\n",
    "def classify(using):\n",
    "    arr=np.zeros((14,75))\n",
    "    for col in range(14):\n",
    "        fft=(np.abs(np.fft.hfft(using[:,col]))).tolist()\n",
    "        fft=np.asarray(fft[1:76]) #1-25 hz\n",
    "        arr[col]=fft\n",
    "    arr=arr.flatten()\n",
    "    val=mlr1.predict(arr.reshape(1,-1))[0]\n",
    "    aro=mlr2.predict(arr.reshape(1,-1))[0]\n",
    "    print(\"\\n\\nValence: MLP\",val,\"\\tKNN\",knn1.predict(arr.reshape(1,-1))[0])\n",
    "    print(\"Arousal: MLP\",aro,\"\\tKNN\",knn2.predict(arr.reshape(1,-1))[0])\n",
    "    return val,aro\n",
    "    \n",
    "    \n",
    "dispatcher = dispatcher.Dispatcher()\n",
    "dispatcher.map(\"/openbci\", print_test)\n",
    "unity=udp_client.SimpleUDPClient(\"127.0.0.1\", 5555)\n",
    "    \n",
    "async def loop():\n",
    "    while True:\n",
    "        curr=np.asarray(vals)\n",
    "        if (curr.shape[0]>=375):\n",
    "            curr=curr[-375:]\n",
    "            valence,arousal=classify(curr)\n",
    "            unity.send_message(\"/python\",[float(valence),float(arousal)])\n",
    "            \n",
    "        await asyncio.sleep(1)\n",
    "\n",
    "\n",
    "async def init_main():\n",
    "    server = osc_server.AsyncIOOSCUDPServer(('127.0.0.1', 12347), dispatcher, asyncio.get_event_loop())\n",
    "    transport, protocol = await server.create_serve_endpoint()  # Create datagram endpoint and start serving\n",
    "\n",
    "    await loop()  # Enter main loop of program\n",
    "\n",
    "    transport.close()  # Clean up serve endpoint\n",
    "\n",
    "\n",
    "await init_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence=123\n",
    "unity=udp_client.SimpleUDPClient(\"127.0.0.1\", 5555)\n",
    "unity.send_message(\"/python\",valence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Dataset_1/EEG/CSV/Part1_IAPS_SES1_EEG_fNIRS_03082006.csv')\n",
    "        #df=df.drop(['F5','F8','AF7','AF8','AFz','# Fp1','Fp2','Fpz','F7','F6'],axis=1)#10 electrodes are not used\n",
    "        #if participant=='1' and session==1:\n",
    "         #   df=df.drop(['EXG1','EXG2','EXG3','EXG4','EXG5','EXG6','EXG7','EXG8','Status'],axis=1)\n",
    "        #else:\n",
    "         #   df=df.drop(['GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status'],axis=1)\n",
    "df=df[['AF3', 'FT7','F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'FT8', 'AF4']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Old way\n",
    "\n",
    "def makecsv(participant,dates,refresh):\n",
    "    #values=[]\n",
    "    for session in range(1,4):\n",
    "        curr=classes[:,session-1]\n",
    "\n",
    "        #Read self assesments (valence-arousal)\n",
    "        file=open('Dataset_1/Common/Part'+participant+'SES'+str(session)+'.log','r')\n",
    "        score=re.split('\\n\\n|\\n',file.read())\n",
    "        i=0\n",
    "        scores=[]\n",
    "        for s in score:\n",
    "            i=i+1\n",
    "            if i%5==0:\n",
    "                scores.append(list(map(float,(s.split(' '))[-2:])))\n",
    "        scores=np.asarray(scores)\n",
    "\n",
    "        #Read markers\n",
    "        file=open('Dataset_1/EEG/Part'+participant+'_IAPS_SES'+str(session)+'_EEG_fNIRS_0'+dates[session-1]+'082006.bdf.mrk','r')\n",
    "        tags=file.read().split('\\t')\n",
    "        tag=[]\n",
    "        i=0\n",
    "        for t in tags[:-3]:\n",
    "            if t[0].isdigit():\n",
    "                i=i+1\n",
    "                if i%4==0:\n",
    "                    tag.append(int(t))\n",
    "\n",
    "        marks=(sorted(set(tag)))\n",
    "\n",
    "        #Read CSV\n",
    "        df=pd.read_csv('Dataset_1/EEG/CSV/Part'+participant+'_IAPS_SES'+str(session)+'_EEG_fNIRS_0'+dates[session-1]+'082006.csv')\n",
    "        df=df.drop(['F5','F8','AF7','AF8','AFz','# Fp1','Fp2','Fpz','F7','F6'],axis=1)#10 electrodes are not used\n",
    "        if participant=='1' and session==1:\n",
    "            df=df.drop(['EXG1','EXG2','EXG3','EXG4','EXG5','EXG6','EXG7','EXG8','Status'],axis=1)\n",
    "        else:\n",
    "            df=df.drop(['GSR1','GSR2','Erg1','Erg2','Resp','Plet','Temp','Status'],axis=1)\n",
    "\n",
    "\n",
    "        #Get FFTS of the first 3 seconds of each trigger. We take the FFT for every 2 seconds\n",
    "        rate=refresh[session-1]\n",
    "        i=marks[0]\n",
    "        for m in marks:\n",
    "            i=m\n",
    "            ind=marks.index(m)\n",
    "            for s in range (2):\n",
    "                e=i+refresh[session-1]\n",
    "                d=df[i:e+refresh[session-1]]\n",
    "                arr=np.zeros((54,50))\n",
    "                j=0\n",
    "                for col in list(d):\n",
    "                    fft=(np.abs(np.fft.hfft(d[col]))).tolist()\n",
    "                    #del fft[1::2]\n",
    "                    fft=np.asarray(fft[1:51]) #1-25 hz\n",
    "                    arr[j]=fft\n",
    "                    j=j+1\n",
    "\n",
    "                #Mean and normalize values\n",
    "                vals=np.mean(arr,axis=0)\n",
    "                #vals=vals/np.sum(vals)\n",
    "                vals=vals.tolist()\n",
    "                vals.append(scores[ind][0])\n",
    "                vals.append(scores[ind][1])\n",
    "                #vals.append(curr[ind])\n",
    "                values.append(np.asarray(vals).flatten())\n",
    "                i=e\n",
    "\n",
    "    #np.savetxt(\"Participant\"+participant+\".csv\",np.asarray(values),delimiter=\",\",fmt='%s')\n",
    "    \n",
    "#Read Classes of images\n",
    "file=open('Dataset_1/Common/IAPS_Classes_EEG_fNIRS.txt','r')\n",
    "classes=re.split('\\t|\\n',file.read())\n",
    "new=[(classes[:-1])[i:i+3] for i in range(0, len(classes)-1, 3)]\n",
    "classes=np.asarray(new)\n",
    "\n",
    "\n",
    "values=[]\n",
    "#Participants\n",
    "#1\n",
    "dates=['3','7','8']\n",
    "refresh=[256,1024,1024]\n",
    "makecsv('1',dates,refresh)\n",
    "\n",
    "#2\n",
    "dates=['7','8','9']\n",
    "refresh=[1024,1024,1024]\n",
    "makecsv('2',dates,refresh)\n",
    "\n",
    "#3\n",
    "dates=['7','8','8']\n",
    "makecsv('3',dates,refresh)\n",
    "\n",
    "\n",
    "#4\n",
    "dates=['7','8','9']\n",
    "makecsv('4',dates,refresh)\n",
    "\n",
    "\n",
    "#5\n",
    "dates=['8','9','9']\n",
    "makecsv('5',dates,refresh)\n",
    "\n",
    "np.savetxt(\"Dataset1Separate.csv\",np.asarray(values),delimiter=\",\",fmt='%s')\n",
    "\n",
    "\n",
    "#DATASET 2 FORMATTING\n",
    "values=[]\n",
    "for part in range(1,24):\n",
    "    #values=[]\n",
    "    for vid in range (1,19):\n",
    "        df=pd.read_csv('Dataset_2/MatlabOutput/Part'+str(part)+'Vid'+str(vid)+'.csv',header=None).values\n",
    "        score=pd.read_csv('Dataset_2/MatlabOutput/Part'+str(part)+'Vid'+str(vid)+'Score.csv',header=None).values[0]\n",
    "        #Removing DC Offset\n",
    "        for x in range (df.shape[1]):\n",
    "            m=np.mean(df[:,x])\n",
    "            df[:,x]=df[:,x]-m\n",
    "        start=500\n",
    "        for x in range (10):\n",
    "            end=start+128\n",
    "            d=df[start:(end+128),:]\n",
    "            arr=np.zeros((14,75))\n",
    "            for col in range(14):\n",
    "                fft=(np.abs(np.fft.hfft(d[:,col]))).tolist()\n",
    "                #del fft[1::2]\n",
    "                fft=np.asarray(fft[1:76]) #1-25 hz\n",
    "                arr[j]=fft\n",
    "            print('Valence:',knn1.predict(test[x].reshape(1,-1))\n",
    "            print('Arousal:',knn2.predict(test[x].reshape(1,-1))\n",
    "                \n",
    "            #Mean and normalize values\n",
    "            vals=np.mean(arr,axis=0)\n",
    "            #vals=vals/np.sum(vals)\n",
    "            vals=vals.tolist()\n",
    "            vals.append(score[0])\n",
    "            vals.append(score[1])\n",
    "            #vals.append(curr[ind])\n",
    "            values.append(np.asarray(vals).flatten())\n",
    "            start=end\n",
    "    #np.savetxt(\"Participant\"+str(part)+\".csv\",np.asarray(values),delimiter=\",\",fmt='%s')\n",
    "    \n",
    "np.savetxt(\"Dataset2Separate.csv\",np.asarray(values),delimiter=\",\",fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
